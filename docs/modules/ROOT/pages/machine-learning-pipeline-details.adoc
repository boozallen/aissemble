= Machine Learning Pipeline Patterns

== Overview
To simplify the incorporation of commonly used pipeline components, pre-constructed patterns have been developed and can
be included in your project. This means that if you are adding a machine learning pipeline to your project, there are
only a few steps necessary to incorporate generated code for some major machine-learning muscle movements such as
training and inference. This page is intended to assist in understanding the generated components that are included when
using these patterns as well as assist you in determining where to modify and customize elements to suit your specific
implementation.

== What Gets Generated
For projects that have included a pre-fab machine learning pipeline, several pre-constructed elements are generated when
the project is initially built. These elements are intended to simplify the inclusion of infrastructure and scaffolding
in the project, reducing the burden on project teams to each develop and incorporate these elements. Below are some
examples of generated elements.

=== JSON
The JSON file that is generated includes details about the machine learning pipeline implemented for the project,
including the specification of steps that comprise the pipeline and other details. (for information on configuring the
json file, please check out xref:pipeline-metamodel.adoc[Detailed Pipeline Options])

.ExampleMachineLearningPipeline.json
[source,json]
----
{
	"name": "ExampleMachineLearningPipeline",
	"package": "com.boozallen.aissemble",
	"type": {
		"name": "machine-learning",
		"implementation": "machine-learning-mlflow"
	},
	"steps": [
		{
			"name": "AissembleMachineLearningTraining",
			"type": "training",
			"inbound": {
				"type": "messaging",
				"channelName": "train"
			}
		},
		{
			"name": "AissembleMachineLearningInference",
			"type": "inference"
		}
	]
}
----

//todo section seems redundant with previous documentation on the mda generation build-action-build-action loop
=== POM
After specifying the contents of the JSON file, you would then run a `mvn clean install` to generate the associated POM
file. An example POM file associated with the example JSON from above is shown below. Some content in the pom file
will be specific to the machine learning pipeline defined for the project.

.example-pom.xml
[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<parent>
		<groupId>com.boozallen.aissemble</groupId>
		<artifactId>my-solution-baseline-project</artifactId>
		<version>1.0.0-SNAPSHOT</version>
	</parent>

	<artifactId>example-machine-learning-pipeline</artifactId>
	<packaging>pom</packaging>

	<name>My Solution Baseline Project::Pipelines::Example Machine Learning Pipeline</name>
	<description>${pipeline.description}</description>

	<modules>
        <!-- TODO: replace with your step-specific modules here -->
	</modules>

	<build>
		<plugins>
			<plugin>
				<groupId>org.technologybrewery.fermenter</groupId>
				<artifactId>fermenter-mda</artifactId>
				<configuration>
					<basePackage>com.boozallen.aissemble</basePackage>
					<profile>machine-learning-pipelines</profile>
					<propertyVariables>
						<targetPipeline>ExampleMachineLearningPipeline</targetPipeline>
					</propertyVariables>
					<!-- see my-solution-baseline-project-pipelines for base configuration settings -->
				</configuration>
			</plugin>
		</plugins>
	</build>

</project>
----


As an output of the build, you will be prompted to add the machine learning module to the pipelines identified in your
project. An excerpt from the build output is shown below. This excerpt instructs you to add the pipeline module to
activate it in your build.

.Manual Action Excerpt
----
[WARNING] 
***********************************************************************
*** MANUAL ACTION NEEDED!                      ***
***********************************************************************
You must add the pipeline module to my-solution-baseline-project-pipelines/pom.xml to activate it in your build!

Example:
  <modules>
    <module>example-machine-learning-pipeline</module>
    <!-- Just add the above module if the <modules> tag already exists -->
  </modules>

***********************************************************************
***********************************************************************
----

You will then run another `mvn clean install` to activate the pipeline and generate out the training and inference
module pom files. As an output of the build, you will be prompted to add the training and inference modules to the
machine learning pipeline identified in your project. This excerpt is shown below.

.Manual Action Excerpt
----
[WARNING] 
***********************************************************************
*** MANUAL ACTION NEEDED!                      ***
***********************************************************************
You must add the step module to example-machine-learning-pipeline/pom.xml to activate it in your build!

Example:
  <modules>
    <module>aissemble-machine-learning-training</module>
    <!-- Just add the above module if the <modules> tag already exists -->
  </modules>

***********************************************************************
***********************************************************************
[WARNING] 
***********************************************************************
*** MANUAL ACTION NEEDED!                      ***
***********************************************************************
You must add the step module to example-machine-learning-pipeline/pom.xml to activate it in your build!

Example:
  <modules>
    <module>aissemble-machine-learning-inference</module>
    <!-- Just add the above module if the <modules> tag already exists -->
  </modules>

***********************************************************************
***********************************************************************
----

The next step you will need to do is add in each of the steps listed in the example output into the machine learning
pipeline pom. (e.g. `<module>aissemble-machine-learning-training</module>`,
`<module>aissemble-machine-learning-inference</module>`). Once completed, a final `mvn clean install` is run to
generate the scaffolding code for inference and training.

=== Inference
For our example, under the generated `aissemble-machine-learning-inference` module, the following will be generated
and will require some updates:

.Inference Items to Update
[cols="2,3a"]
|===
|File Name/Directory|Notes/Updates

|`src/aissemble_machine_learning_inference/inference_impl.py`
|This file is where the majority of inference model loading and prediction execution will be implemented.

* Add imports that are needed for your chosen type of machine learning
* Load your model using the mlflow package that corresponds to your implementation
* Add in tensorflow if you need it
* Map predictions to the desired inference format

|`src/aissemble_machine_learning_inference/inference/rest/inference_api_rest.py`
|Defines the https://fastapi.tiangolo.com/[FastAPI,role=external,window=_blank] based REST API that is exposed to
inference consumers.  Developers may modify this file as needed to customize the API, which may include adding
authentication/authorization, enabling CORS, etc.

|`pyproject.toml`
|All Python projects emitted by aiSSEMBLE are https://python-poetry.org/[Poetry,role=external,window=_blank] projects
that rely on https://github.com/TechnologyBrewery/habushu[Habushu,role=external,window=_blank] for DevOps automation.
`pyproject.toml` is a required Poetry configuration that aligns with
https://peps.python.org/pep-0518/[PEP-518,role=external,window=_blank] and defines the project's build system
requirements.  Developers should customize this configuration as needed to support additional dependencies or
capabilities required within pipelines.

|`src/aissemble_machine_learning_inference/validation/inference_payload_definition.py`
|Defines the request/response payloads that capture the format of input data records for inferencing as well as the
output format of inference results.

* You will need to make sure all expected fields that you will be using are defined here for inference requests.
Developers may also define inference payloads by specifying `inbound`/`outbound` elements within the corresponding
`inference` pipeline metamodel that reference semantic type dictionary records.

|`src/aissemble_machine_learning_inference/resources/krausening/base/inference.properties`
|This can be updated to configure different properties for your inference build

* Update the model_directory to match the path to your model.
* Add any additional properties that you may need

|`src/aissemble_machine_learning_inference/config/inference_config.py`
|This file pulls in the configurations for inference. No updates should be needed, but if you have any custom
configurations you need to setup, this file can be updated.

|`tests/features/inference.feature`
|This file comes with a basic "Hello World" feature that shows a quick example of a feature written in Gherkin. For
every feature you are trying to support and have test steps to correspond, we recommend you write a feature in this file.

* Remove the base Hello world feature (if you have other features to add)
* Add features that you need to test

|`src/test/python/features/steps/inference_steps.py`
|This is where you can write your test code that will link up to the features written in the inference.feature file

* Remove the "Hello World" steps if the feature was removed from the feature file.
* Add methods for each step that you have in your feature file
|===


=== Training
Under the `aissemble-machine-learning-training` module, the following will be generated:

.Training Items to Update
[cols="2,4a"]
|===
|File Name/Directory|Notes/Updates

|`src/aissemble_machine_learning_training/impl/example_machine_learning_pipeline.py`
|This will likely be the place where most of your work will occur. This is where you will be writing each of your
implementation code for each of the main methods:

* acknowledge_training_alert (optional)
* load_dataset
* prep_dataset
* select_features
* split_dataset
* train_model
* evaluate_model
* save_model
* deploy_model

|`src/aissemble_machine_learning_training/example_machine_learning_pipeline_driver.py`
|By default, this driver listens for the appropriate training alert and executes the training pipeline.  Developers may
update this driver based on the specifics of how training jobs should be triggered.

|`pyproject.toml`
|All Python projects emitted by aiSSEMBLE are https://python-poetry.org/[Poetry,role=external,window=_blank] projects
that rely on https://github.com/TechnologyBrewery/habushu[Habushu,role=external,window=_blank] for DevOps automation.
`pyproject.toml` is a required Poetry configuration that aligns with
https://peps.python.org/pep-0518/[PEP-518,role=external,window=_blank] and defines the project's build system requirements.  Developers should customize this configuration as needed to support additional dependencies or capabilities required within pipelines.

|`src/aissemble_machine_learning_training/generated`
|This is where your generated code will live. Nothing should be updated within this folder.

|`src/aissemble_machine_learning_training/config/pipeline_config.py`
|This file pulls in the configurations for inference. No updates should be needed, but if you have any custom
configurations you need to set up, this file can be updated.

|`src/aissemble_machine_learning_training/resources/config/pipeline.properties`
|This can be updated to configure different properties for your training build

* Add any additional properties that you may need

|`tests/features/training.feature`
|This file comes with a basic "Hello World" feature that shows a quick example of a feature written in Gherkin. For
every feature you are trying to support and have test steps to correspond, we recommend you write a feature in this file.

* Remove the base Hello world feature (if you have other features to add)
* Add features that you need to test

|`tests/features/steps/training_steps.py`
|This is where you can write your test code that will link up to the features written in the `inference.feature` file

* Remove the "Hello World" steps if the feature was removed from the feature file.
* Add methods for each step that you have in your feature file
|===

[#_sagemaker_training]
=== SageMaker Training
Under the `aissemble-machine-learning-sagemaker-training` module, the following will be generated:

.Training Items to Update
[cols="2,4a"]
|===
|File Name/Directory|Notes/Updates

|`src/aissemble_machine_learning_sagemaker_training/example_machine_learning_pipeline.py`
|This will likely be the place where most of your work will occur. This is where you will be writing each of your
implementation code for each of the main methods:

* load_train_data
* load_validation_data
* load_test_data
* train_model
* validate_model
* test_model
* save_model

At runtime, SageMaker will mount files to known locations which are set as constants within the training script template.
These files contain the hyperparameters, input data configuration, and input datasets. SageMaker also expects that the
resulting model artifacts are written out to a particular location, and that if any errors occur those are written out
to a particular filepath. All of this logic is provided by default in the training script template, in most cases
modifications won't be necessary.

MLflow provides several standard https://mlflow.org/docs/latest/models.html#built-in-model-flavors/[built-in
flavors,role=external,window=_blank] that contains tools such as logging, evaluating, and saving models for different
machine learning frameworks. Although not required, it is advised to use MLflow Model functionality for your
`save_model` function so that the model can be understood by different downstream tools. If the machine learning
framework is not within the MLflow built-in flavors, you can define your own custom
https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.save_model/[pyfunc
flavor,role=external,window=_blank] to save your model.

|`pyproject.toml`
|All Python projects emitted by aiSSEMBLE are https://python-poetry.org/[Poetry,role=external,window=_blank] projects
that rely on https://github.com/TechnologyBrewery/habushu[Habushu,role=external,window=_blank] for DevOps automation.
`pyproject.toml` is a required Poetry configuration that aligns with
https://peps.python.org/pep-0518/[PEP-518,role=external,window=_blank] and defines the project's build system requirements.  Developers should customize this configuration as needed to support additional dependencies or capabilities required within pipelines.

|`tests/features/training.feature`
|This file comes with a basic "Hello World" feature that shows a quick example of a feature written in Gherkin. For
every feature you are trying to support and have test steps to correspond, we recommend you write a feature in this file.

* Remove the base Hello world feature (if you have other features to add)
* Add features that you need to test

|`tests/features/steps/training_steps.py`
|This is where you can write your test code that will link up to the features written in the `inference.feature` file

* Remove the "Hello World" steps if the feature was removed from the feature file.
* Add methods for each step that you have in your feature file
|===

==== AWS Credentials and AWS Elastic Container Registry Setup

https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html?icmpid=docs_sagemaker_lp/index.html[SageMaker,role=external,window=_blank]
requires the model training image to be pushed to
https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html[AWS ECR,role=external,window=_blank]. To enable
this, you must first create a repository within your ECR registry named
`<project-name>-<sagemaker-training-step-name>-docker` either through the AWS console or by using the AWS CLI. In
addition, you must create a "server" within your Maven settings file (typically located at
`${user.home}/.m2/settings.xml`) and provide your ECR credentials in the following format:

[source]
----
<server>
  <id>REPO_ID</id>
  <username>AWS</username>
  <password>REPO_PASSWORD</password>
</server>
----

In the above snippet:

* Replace `REPO_ID` with any name, but it must match the `REPO_ID` you use in the `pom.xml` file discussed below
* To obtain `password`:
** First https://maven.apache.org/guides/mini/guide-encryption.html#how-to-create-a-master-password[create a master
Maven password,role=external,window=_blank]
** Then run `mvn --encrypt-password $(aws ecr get-login-password)`
*** Please note that you may need to specify region and/or profile as arguments to the nested `aws` command
* The `username` should be `AWS`

==== Pushing Model Training Image to ECR

Finally, in your `<project-name>/<project-name>-docker/<project-name>-<sagemaker-training-step-name>-docker/pom.xml`
file, you must provide your ECR Repo URL, Repo ID (must match the `REPO_ID` provided in your `settings.xml` file), and
add your ECR Repo URL as an image name prefix. You must also set `skip` to `false` to enable building and pushing the
image to ECR. There are comments within that file which will point you towards those required changes.

==== Data Preparation

Please see the xref:pyspark-data-delivery-pipeline-details.adoc#_preparing_data_for_sagemaker_model_training["Preparing Data for SageMaker Model Training"]
section of the PySpark Data Delivery Pipelines page for instructions detailing input data preparation for SageMaker
training jobs.

=== Model Training API

If you include a training step, a Docker build is included to execute your model training logic as a Kubernetes job
within your project cluster.

You can also optionally include a Model Training API in your project, which will allow you to create model training
jobs, list jobs, retrieve job logs, and kill jobs via HTTP requests. By default, this service will be listening on
*port 5001*.

In order to include this API in your project, include an execution in your deployment `pom.xml` pointing to the
`training-deploy` profile. More information is available on the xref:containers.adoc#_containers[Container Support page].

Here are the available routes:

POST /training-jobs?pipeline_step={pipelineStep}

* `pipelineStep` is the name of the "training" ML pipeline step you would like to execute
** Must be CamelCased
* The request body contains all key/value pairs required for model training, such as model hyperparameters
** You are responsible for reading in these hyperparameters within your model training script
* Functionality:
** Spawns appropriate model training Kubernetes job
*** Returns 500 error if pipeline step not present
** Returns model training job name
*** Sample response: "model-training-logistic-training-a8bfa39b-aa2b-403c-8311-f40dda"

GET /training-jobs/{trainingJobName}

* Returns logs from pod running model training job
* Returns 400 error if job doesn't exist

GET /training-jobs

* Returns list of all model training jobs (active, failed, and completed) and statuses
* Returns 500 error if training jobs statuses cannot be retrieved
* Sample response:
[source]
----
[{'name': 'model-training-logistic-training-d20dd35d-910e-4bb0-8862-621ce7',
  'status': "{'active': None,\n"
            " 'completed_indexes': None,\n"
            " 'completion_time': None,\n"
            " 'conditions': [{'last_probe_time': datetime.datetime(2023, 5, "
            '10, 6, 41, 51, tzinfo=tzlocal()),\n'
            "                 'last_transition_time': datetime.datetime(2023, "
            '5, 10, 6, 41, 51, tzinfo=tzlocal()),\n'
            "                 'message': 'Job has reached the specified "
            "backoff limit',\n"
            "                 'reason': 'BackoffLimitExceeded',\n"
            "                 'status': 'True',\n"
            "                 'type': 'Failed'}],\n"
            " 'failed': 1,\n"
            " 'ready': 0,\n"
            " 'start_time': datetime.datetime(2023, 5, 10, 6, 41, 47, "
            'tzinfo=tzlocal()),\n'
            " 'succeeded': None,\n"
            " 'uncounted_terminated_pods': {'failed': None, 'succeeded': "
            'None}}'}]
----


GET /training-jobs?pipeline_step={pipelineStep}

* `pipelineStep` is the name of the "training" ML pipeline step you would like to execute
** Must be CamelCased
* For the given pipeline step, returns list of all model training jobs (active, failed, and completed) and statuses
* Returns 500 error if training jobs statuses cannot be retrieved
* Sample response:
[source]
----
[{'name': 'model-training-logistic-training-d20dd35d-910e-4bb0-8862-621ce7',
  'status': "{'active': None,\n"
            " 'completed_indexes': None,\n"
            " 'completion_time': None,\n"
            " 'conditions': [{'last_probe_time': datetime.datetime(2023, 5, "
            '10, 6, 41, 51, tzinfo=tzlocal()),\n'
            "                 'last_transition_time': datetime.datetime(2023, "
            '5, 10, 6, 41, 51, tzinfo=tzlocal()),\n'
            "                 'message': 'Job has reached the specified "
            "backoff limit',\n"
            "                 'reason': 'BackoffLimitExceeded',\n"
            "                 'status': 'True',\n"
            "                 'type': 'Failed'}],\n"
            " 'failed': 1,\n"
            " 'ready': 0,\n"
            " 'start_time': datetime.datetime(2023, 5, 10, 6, 41, 47, "
            'tzinfo=tzlocal()),\n'
            " 'succeeded': None,\n"
            " 'uncounted_terminated_pods': {'failed': None, 'succeeded': "
            'None}}'}]
----

DELETE /training-jobs/{trainingJobName}

* Deletes specified Kubernetes job
* Returns 500 error if job does not exist
* Sample Response: "model-training-logistic-training-a8bfa39b-aa2b-403c-8311-f40dda successfully deleted."


[#_sagemaker_model_training_api]
==== Sagemaker Model Training API

The SageMaker Training API is component that allows you to facilitate model training using AWS SageMaker. 
If you include a sagemaker training step, a Docker build is included to execute your model training logic as a
Kubernetes job within your project cluster.

Here are the available routes:

POST /sagemaker-training-jobs

* Submits a new model training job to AWS SageMaker using the specified 
* Parameters:
** image_uri (string): The URI of the container image to use for training.
** hyperparameters (dictionary): A dictionary of hyperparameters for the model training job.
** instance_type (string): The EC2 instance type to use for training the model.
** bucket (string): The name of the S3 bucket where the training data is stored.
** prefix (string): The prefix path within the S3 bucket where the training data is located.
** metric_definitions (dict): A dictionary of metrics to be monitored during training. Keys are metric names, and
values are corresponding Regular Expressions (RegEx) which will be used to parse out your desired metrics from your
model training container's output during runtime.
* Submits a new SageMaker training job with the provided configurations. The training data is expected to be located in
S3 with the provided bucket and prefix.
* Returns the name of the latest training job as a response.
* A background task will be spawned performing the following:
** An MLflow experiment run will be created with the same name as the training job.
** Metrics will be retrieved from the SageMaker API and populated in MLflow during training.
** If the job fails, a failure message will be logged to MLflow as an "error.txt" file.
** If the job succeeds, resulting model artifacts will be downloaded and logged to MLflow.

GET /{sagemaker_job_name}

* Retrieves the status of a specific SageMaker training job.
* Parameters:
** sagemaker_job_name (string): The name of the SageMaker training job to retrieve the status for.
* Returns a JSON response containing the job name and its status.
* Returns 500 error if jobs statuses cannot be retrieved.


[#_sagemaker_model_training_authentication]
==== Sagemaker Model Training Authentication


*Authentication for AWS Access:*
To use the provided code and authenticate with AWS, the user needs to have AWS credentials. These credentials are used
to identify and authenticate the user's access to AWS services.

*Where to Provide AWS Credentials:*
Environment Variables via Helm Chart: Users can provide AWS credentials as environment variables through the Helm chart
by modifying the `values.yaml` file within the `model-training-api-sagemaker` deployment configuration. The code will
then access and utilize the credentials provided via these environment variables set within the Helm chart.
[source,yaml]
----
env:
    ...
    - name: AWS_ACCESS_KEY_ID
      value: YOUR_AWS_ACCESS_KEY_ID
    - name: AWS_SECRET_ACCESS_KEY
      value: YOUR_AWS_SECRET_ACCESS_KEY
    - name: AWS_SESSION_TOKEN
      value: YOUR_AWS_SESSION_TOKEN
    - name: AWS_DEFAULT_REGION
      value: YOUR_AWS_DEFAULT_REGION
    - name: AWS_ROLE_ARN
      value: YOUR_AWS_ROLE_ARN
----

*Finding AWS Credentials:*
Before setting up the AWS credentials for your deployment, you'll need to ensure you have the necessary AWS credentials
to authenticate your application. Here's how you can obtain these credentials:

* Access Key ID and Secret Access Key:
** Log in to your AWS Management Console. Open the "Services" menu and select "IAM". In the IAM dashboard, select
"Users" from the left-hand menu. Choose the user whose credentials you want to use and navigate to the "Security
credentials" tab. Under the "Access keys" section, you can create or manage access keys. Make sure to note down the
Access Key ID and Secret Access Key.

* Session Token:
** If you are using temporary security credentials, you might need a Session Token. Refer to
https://docs.aws.amazon.com/iam/index.html[AWS documentation,role=external,window=_blank] or your organization's
guidelines on how to obtain temporary security credentials and Session Tokens.

* AWS Region:
** You can find the AWS region information in the AWS Management Console.

* AWS_ROLE_ARN:
** The `AWS_ROLE_ARN` should follow a specific pattern: `arn:aws:iam::account-id-without-hyphens:role/role-name`. For
role-name, "AmazonSageMaker-ExecutionRole" is a managed AWS Identity and Access Management (IAM) role that is
specifically designed and recommended for use with Amazon SageMaker.