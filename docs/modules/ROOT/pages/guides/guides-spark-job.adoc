[#_configuring_and_executing_spark_jobs]
= Configuring and Executing Spark Jobs

== Overview
aiSSEMBLE(TM) leverages https://github.com/kubeflow/spark-operator[Spark Operator] for all
in-cluster executions of data flow pipelines.  This mechanism is a simple, consistent, efficient mechanism for both
triggering a pipeline and managing its lifecycle within the cluster.  Running your pipeline is
as simple as configuring a YAML file (the Spark Application Specification), and submitting it to the
Kubernetes cluster. Spark Operator will automatically detect the SparkApplication, and configure,
deploy, and execute the data flow pipeline as a Spark job.

.Spark Operator Architecture Diagram (credit: KubeFlow Spark Operator repository)
image::https://github.com/kubeflow/spark-operator/blob/master/docs/architecture-diagram.png?raw=true["Spark Operator Architecture"]

This guide will cover how to configure your data flow pipeline's deployment, which deployments are core to
the aiSSEMBLE Spark ecosystem, and how to execute and interact with your pipeline within a Kubernetes cluster.

[#_configuration]
== Configuration

Almost all configuration of your pipeline's execution and environment will be conducted via the Spark Application file,
which aiSSEMBLE will automatically xref:data-delivery-pipeline-overview.adoc#_spark_application_yaml[generate].
This file is submitted directly to the Spark Operator, and follows a standard, well-documented
structure and specification.  This specification includes configuration of CPU and memory resources, environment
variables, dependencies, and more.  Exhaustive documentation is available from the
https://github.com/kubeflow/spark-operator/blob/master/docs/user-guide.md#writing-a-sparkapplication-spec[
open source Spark Operator project page].

[TIP]
PySpark jobs are configured and run the same as Spark jobs!

aiSSEMBLE will generate multiple versions of the `SparkApplication` file, with each leveraged at various stages of
testing and deployment.  Each version may be configured as needed based on its role and usage.  Cluster-wide
Spark configurations may be provided through a reference to a deployed
https://kubernetes.io/docs/concepts/configuration/configmap/[ConfigMap].  By default, aiSSEMBLE will assign
a small set of configurations in the `spark-config` `ConfigMap`, defined in the `spark-infrastructure` Helm chart.

=== Additional Environmental Configuration
For advanced cases that require more direct control over the execution environment, additions may be made to the
`spark-worker-docker` docker image.  This image serves as the environment for all data flow pipeline executions.  Making
modifications here may prove useful for adding specific local resources, such as a shell script, an executable binary,
or additional dependencies that cannot be assigned via the `SparkApplication`.

== Deployment Structure
There are several deployments involved in aiSSEMBLE's deployed Spark ecosystem.  Each can be configured as needed
through their own mechanisms, most frequently Helm charts.

|===
|Deployment |Purpose

|`spark-operator`
|Core mechanism for directing Spark Job execution within the Kubernetes cluster.

|`spark-operator-webhook-init`
|Component of internal Spark Operator functionality.

|`spark-operator-webhook-cleanup`
|Component of internal Spark Operator functionality.

|`spark-infrastructure`
|Hosts Spark History, Hive Thrift Server, and Hive metastore Deployment shared by all Spark pipelines and executions.

|`s3-local`
|Default shared storage solution from which Spark executors can read and write.

|`airflow`
|Optional xref:pipeline-metamodel.adoc#_pipeline_type_element_options[submission mechanism] for SparkApplication
objects.
|===

== Executing your Data Delivery Pipeline
To trigger the execution of a Spark Job, a `SparkApplication` is submitted to the cluster.  aiSSEMBLE
provides multiple mechanisms for `SparkApplication` submission (detailed in the following sections).
Once submitted, the Spark Operator deployment will detect the `SparkApplication`, create the
appropriate pods within the Kubernetes cluster, and begin the job execution.

NOTE: Once a `SparkApplication` is submitted to the cluster, it can take several seconds for Spark Operator to detect
it and begin execution.  This is normal and expected.

When the pipeline terminates, Spark Operator will tear down the pods and release any reserved resources.

=== For Local Cluster Testing via Tilt

After deploying with Tilt, a resource is visible for each data delivery pipeline.  Once all other resources are ready,
use the trigger button
image:https://raw.githubusercontent.com/tilt-dev/tilt/542020055dd9629ce1122957d162be619184ccfd/web/src/assets/svg/start-build-button-manual.svg[width=25, alt=""]
on the pipeline resource to submit the associated `SparkApplication` to the cluster.

Select the triggered resource to view the associated logging.

=== Triggering a Job via Airflow

[NOTE]
Only available if Airflow is identified as an xref:pipeline-metamodel.adoc#_pipeline_type_element_options[`executionHelper`]
in your pipeline model.

Once logged into your https://airflow.apache.org/docs/apache-airflow/stable/index.html[Airflow] deployment, you
will see that aiSSEMBLE has automatically created
https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html[DAGs (Directed Acyclic Graphs)] for
each of your data flow pipelines.  To execute a pipeline, trigger its associated DAG.

Logs can be viewed from the
https://airflow.apache.org/docs/apache-airflow/stable/ui.html#task-instance-context-menu[task context menu] for
the spark_monitor task.

[WARNING]
Airflow may only show the job's logging after it has completed execution.

=== Triggering a Job Through kubectl
[WARNING]
Avoid this method in production environments due to security and direct access concerns.

From a terminal with access to your Kubernetes cluster, we can leverage
https://kubernetes.io/docs/reference/kubectl/[`kubectl`] to submit the `SparkApplication`directly.  To accomplish this,
identify the path to the `SparkApplication` YAML file that you would like to submit. A resolved `SparkApplication`
deployment chart is written to your pipeline's target directory as target/Chart.yaml. From there, you need only run a
single command:

[source]
----
kubectl apply -f <path-to-chart-yaml>
----

To resubmit the same `Spark Application` again, without changing the file's contents, you must delete the
prior submission from the cluster.  Run the following command to perform this action:

[source]
----
kubectl delete -f <path-to-chart-yaml>
----

To retrieve execution logs, we leverage a final `kubectl` command:

[source]
----
kubectl logs <pipeline-name>-driver
----
