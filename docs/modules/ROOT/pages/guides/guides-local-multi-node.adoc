= Running a Multi-Node Cluster Locally

== Overview
Using https://k3d.io[k3d,role=external,window=_blank], a lightweight wrapper to run Rancherâ€™s minimal Kubernetes
distribution https://github.com/k3s-io/k3s[k3s,role=external,window=_blank], we can run a multi-node cluster within
Docker for local development. This allows us to verify that we're able to execute particular components on particular
nodes or node types without necessarily needing access to a cluster with multiple physical nodes. One use case is the
execution of machine learning inference and training components on nodes with GPU-acceleration available. This
technique also allows us to verify that scaling is occurring appropriately. For example, we can verify that PySpark
data delivery pipeline workers are scaling horizontally to improve throughput.

=== Setup
1. In Rancher Desktop settings, set Container Runtime to `dockerd(moby)`
2. Install k3d choosing from the following options:
* `wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash`
* `curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash`
* *macOS:* `brew install k3d`
3. Create multi-node cluster with local dedicated registry, set `agents` argument to desired number of nodes:
* `k3d cluster create <cluster-name> --agents <num-agents> --registry-create <registry-name>`
* Example: `k3d cluster create three-node-cluster --agents 3 --registry-create cluster-registry`
4. Point `kubectl` to your new cluster
* `kubectl config use-context <cluster-name>`
* Example: `kubectl config use-context three-node-cluster`

=== Assigning aiSSEMBLE(TM) Components to Particular Nodes or Node Types
The recommended approach to assigning pods to nodes is using Kubernetes
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity[Node
Affinity,role=external,window=_blank], which allows you to define node selection logic. Within your aiSSEMBLE project,
Helm charts which define Kubernetes deployments for the various components of your project can be found within the
[source]
----
<project-name>-deploy/src/main/resources/apps
----

directory. To define Node Affinity rules for a particular component, navigate to the
[source]
----
<project-name>-deploy/src/main/resources/apps/<component-name>/templates/deployment.yaml
----

template and add an `affinity` section directly under the `spec.template.spec` section. Detailed information regarding
how to configure this section to contain hard/required and soft/preferred affinity rules can be found
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity[here,role=external,window=_blank].
To run a simple example, you can use the following configuration requiring your component to be executed on a node with
the label key/value pair `"label-key":"label-value"`:

[source,yaml]
----
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: label-key
                operator: In
                values:
                - label-value
----

Once you've configured your rules, you need to label your node(s) accordingly. Consult the
https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node[Kubernetes
pod documentation,role=external,window=_blank] for information on how to do so. Continuing with our simple example,
perform the following steps:

1. Retrieve node names by running: `kubectl get nodes`
2. Choose one of the nodes with `agent` in its name
3. Add label to node: `kubectl label nodes <node-name> node-label-key=node-label-value`
* Example: `kubectl label nodes k3d-three-node-cluster-agent-0 label-key=label-value`

Now that you've configured your Node Affinity rules and labeled your node(s), you can run `tilt up` from the root
directory of your aiSSEMBLE project to deploy your project to your multi-node cluster. Finally, to verify that your
rules are being followed, you can run the following command to see which pods are running on which nodes:

[source]
----
kubectl get pods -o wide --field-selector spec.nodeName=<node-name>
----


=== Configuring Horizontal Pod Autoscaling 
Using https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics[Kubernetes
Horizontal Pod Autoscaling,role=external,window=_blank] (HPA), we can configure automatic scaling of deployed
components to meet demand.

To run a simple example, choose a component within your aiSSEMBLE project and add a file named `hpa.yaml` within the
following directory:

[source]
----
<project-name>-deploy/src/main/resources/apps/<component-name>/templates/
----

with the following contents:

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: <component-name>
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: <component-name>
  minReplicas: 5
  maxReplicas: 5
----

If you haven't already, run `tilt up` from the root directory of your project. After a few seconds, 5 pods should be
spun up for your chosen component distributed across the nodes of your cluster. Please note that if you configured
any Node Affinity logic for this component, you may need to remove that logic in order to ensure that your pods can
be scheduled on all of the nodes within your cluster. To verify that 5 pods have been spun up and scheduled across
your various cluster nodes, you can run the following command for each of the nodes in your cluster to see which
pods are running on which nodes:

`kubectl get pods -o wide --field-selector spec.nodeName=<node-name>`