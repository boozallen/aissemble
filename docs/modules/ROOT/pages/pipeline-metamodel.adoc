[#_pipeline_metamodel]
= Pipeline Metamodel

The pipeline metamodel enables data engineers, machine learning engineers, and DevSecOps engineers to specify key
attributes that describe a data delivery, data preparation, or machine learning pipeline. Specifically, this
metamodel allows common pipeline patterns to be quickly instantiated with one or more steps that each detail inbound and
outbound data mapping, persistence definition, provenance, and alerting features. This metamodel is meant to be a
reference and not a complete guide to creating your pipeline.  To view a more specific example, please check out the
core components page that best aligns to your projectâ€™s needs:

* xref:data-delivery-pipeline-overview.adoc[Data Delivery Pipelines]

* xref:machine-learning-pipeline-details.adoc[Machine Learning Pipelines]

== Pipeline Metamodel Specifications

Each metadata instance should be placed in a file with the same name as your pipeline that lives within the following
directory structure (to initially create this structure, please see xref:archetype.adoc[]):

`<project-name>/<project-name>-pipeline-models/src/main/resources/pipelines`

For example:

`test-project/test-project-pipeline-models/src/main/resources/pipelines/TaxPayerPipeline.json`

=== Pipeline Root Element Options
The following options are available on the root pipeline element:

.Pipeline Root Location
[source,json]
----
{
    "..."
}
----

.Pipeline Root Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `name`
| Yes
| None
| Specifies the name of your pipeline. It should represent the functional purpose of your pipeline and must use 
UpperCamelCase/PascalCase notation (e.g., `TaxPayerPipeline.json`).

| `package`
| Yes
| None
| Provides a namespace that will be leveraged to differentiate between instances and provide language-specific 
namespacing (e.g. package in Java, namespace in XSD). The archetype process will default this to your Maven `groupId`, 
which is a recommended best practice to maintain across your pipelines.

| `description`
| No
| None
| A description of the purpose of the pipeline being specified.

| `type` (xref:#_pipeline_type_options[details])
| Yes
| None
| The set of elements that describes the type of pipeline you are modeling (e.g., Data Delivery via Spark, Data Delivery
via Nifi, Machine Learning via MLFlow).

| `dataLineage`
| No
| None
| A flag indicating whether the pipeline should include OpenLineage metadata capture as the
xref:data-lineage.adoc[data lineage] tool.

| `fileStores` (xref:#_pipeline_file_store_options[details])
| No
| None
| The xref:file-storage-details.adoc[file stores] used by the pipeline.

| `steps` (xref:#_pipeline_step_options[details])
| Yes
| None
| The various steps or stages in your pipeline. At least one step is required within steps. Within a Data Delivery
pipeline, each step is a Data Action. Within a Machine Learning pipeline, each step represents part of the ML Workflow.

|===

[#_pipeline_type_options]
=== Pipeline Type Options
The following options are available on the `type` pipeline element:

.Type Location
[source,json]
----
{
    "type": {
        "..."
    }
}
----
.Type Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `type/name`
| Yes
| None
| The name of the pipeline type that is being modeled. Supported types are:

* `data-flow`  - should be used for Data Delivery or Data Preparation pipelines
* `machine-learning`  - should be used for Machine Learning pipelines

| `type/implementation`
| Yes
| None
| The specific implementation of the pipeline type you want to use. Currently supported types are:  

*`data-flow` implementations:*

* xref:spark-data-delivery-pipeline.adoc[`data-delivery-spark`]
* xref:pyspark-data-delivery-pipeline-details.adoc[`data-delivery-pyspark`]

*`machine-learning` implementation:*

* xref:machine-learning-pipeline-details.adoc[`machine-learning-mlflow`]

| `type/platforms` (xref:#_pipeline_type_platform_options[details])
| No
| None
| Any additional platforms to include with the pipeline.

| `type/versioning` (xref:#_pipeline_type_versioning_options[details])
| Yes
| Situational
| By default, `versioning` is not enabled. Specifying this element allows for versioning to be disabled or enabled, if
desired.

| `type/executionHelpers`
| No
| None
| Setting the value to ["airflow"] will bring airflow into the project. Other helpers may be added in the future.

|===


[#_pipeline_type_platform_options]
=== Pipeline Type Platform Options
The following options are available on the `platform` pipeline element:

.Type Platform Location
[source,json]
----
{
    "type": {
        "platforms":[
            {
                "..."
            }
        ]
    }
}
----
.Type Platform Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `type/platforms/platform/name`
| Yes
| None
| The name of an additional platform to include with the pipeline. The following platforms are currently supported:

* `sedona` - adds https://sedona.apache.org/[Apache Sedona,role=external,window=_blank] support for geospatial data
delivery purposes. Note that this platform is only applicable to the `data-delivery-spark` and `data-delivery-pyspark`
pipeline implementations.

|===


[#_pipeline_type_versioning_options]
=== Pipeline Type Versioning Options
The following options are available on the `versioning` pipeline element:

.Type Versioning Location
[source,json]
----
{
    "type": {
        "versioning":{
            "..."
        }
    }
}
----
.Type Versioning Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `type/versioning/enabled`
| No
| None
| Setting `enabled` to false will disable versioning for *machine-learning implementations only*.

|===


[#_pipeline_file_store_options]
=== Pipeline File Store Options
The following options are available on the `fileStore` pipeline element:

.File Store Location
[source,json]
----
{
    "fileStores": [
        {
            "..."
        }
    ]
}
----
.File Store Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `fileStores/fileStore/name`
| Yes
| None
| Specifies the name of your pipeline. It should represent the functional purpose of your pipeline and must use
UpperCamelCase/PascalCase notation (e.g., `PublishedModels`). To be used as the prefix in the pipeline's
xref:file-storage-details.adoc[file stores] configuration.
|===

[#_pipeline_step_options]
=== Pipeline Step Options
The following options are available on the `step` pipeline element:

.Step Location
[source,json]
----
{
    "steps": [
        {
            "..."
        }
    ]
}
----
.Step Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/name`
| Yes
| None
| Specifies the name of your step. It should represent the functional purpose of your step and must use
UpperCamelCase/PascalCase notation (e.g., `IngestNetflowData`).

| `steps/step/type`
| Yes
| None
| Defines the type of step you want to create. There are different options for data-flow and machine-learning pipelines.

*`data-flow` implementations:*

* `synchronous` - the step will accept an input and then return an output in a single execution
* `asynchronous` - the step will listen for messages off a queue and then combine them into batches for subsequent 
processing

*`machine-learning` implementations:*

* `generic` - the step will provide minimal scaffolding for custom logic to be executed in the training/inference steps
* `training` - the step will train an ML model
* `inference` - the step will expose a trained model for inference

| `steps/step/inbound` (xref:#_pipeline_step_inbound_options[details])
| No
| None
| Defines the type of inbound data for the step. If not specified, then there will be no inbound type. Conceptually, 
inbound maps to the parameter type that will be passed to a method/function.

| `steps/step/outbound` (xref:#_pipeline_step_outbound_options[details])
| No
| No
| Defines the type of outbound for the step. If not specified, then there will be no outbound type. Conceptually, 
outbound maps to the parameter type that will be returned from a method/function.

| `steps/step/persist` (xref:#_pipeline_step_persist_options[details])
| No
| None
| Allows specification of the type of persistence that should be performed in this step. If not specified, no persist
logic will be created.

| `steps/step/provenance` (xref:#_pipeline_step_provenance_options[details])
| No
| Yes
| By default, provenance will be tracked for every step. Specifying this element allows for provenance to be disabled or
enabled, if desired

| `steps/step/alerting` (xref:#_pipeline_step_alerting_options[details])
| No
| Yes
| By default, xref:alerting-details.adoc[alerting] is triggered for every step. Specifying this element allows for alerting
to be disabled or enabled, if desired

| `steps/step/postActions` (xref:#_pipeline_step_post_actions_options[details])
| No
| None
| Allows specification of one or more xref:post-actions.adoc[post-training actions] to apply on a `machine-learning`
`training` step *only*. If not specified or specified on a non-training step, no post-training action logic will be
created.

| `steps/step/fileStores`
| No
| None
| A list of xref:file-storage-details.adoc[file store] names utilized by this step. They must be defined in the
`pipeline/fileStores` element.

| `steps/step/configuration` (xref:#_pipeline_step_configuration_options[details])
| No
| None
| Allows the specification of arbitrary list of key-value pairs for implementation-specific configuration.

|===

[#_pipeline_step_inbound_options]
=== Pipeline Step Inbound Options
The following options are available on the `step` pipeline element:

.Step Inbound Location
[source,json]
----
{
    "steps": [
        {
            "inbound": {
                "..."
            }
        }
    ]
}
----
.Step Inbound Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/inbound/type`
| Yes
| None
| Allows specification of how the step will be invoked. There are currently three options:

* xref:messaging-details.adoc[`messaging`] - invocation will occur when a message is received. This message can contain
content, a pointer to content, or just a signal that some process should start.
* `native` - invocation will occur when the step is invoked directly by some caller.
* `void` - no inbound is specified. All other inbound options should be removed if using void or, preferably, the
entire input block should be eliminated from your step.

| `steps/step/inbound/channelName`
| Situational
| None
| Required if using xref:messaging-details.adoc[messaging] as the `inbound` type, `channelName` specifies the messaging
channel from which input should be received.

| `steps/step/inbound/nativeCollectionType` (xref:#_pipeline_step_inbound_native_collection_type_options[details])
| No
| Yes
| If using native as the `inbound` type, `nativeCollectionType` allows the implementation of the collection object being 
passed into the step to be customized to any valid xref:type-metamodel.adoc[Type Manager] type. If not
specified, it will default to dataset (which in turn is defaulted to `org.apache.spark.sql.Dataset` for a Spark-based 
implementation).

| `steps/step/inbound/recordType` (xref:#_pipeline_step_inbound_record_type_options[details])
| No
| Yes
| Allows the type of an individual record being processed in a step to be defined to any valid
xref:type-metamodel.adoc[Type Manager] type. If not specified, it will default to row (which in turn
is defaulted to `org.apache.spark.sql.Row` for a Spark-based implementation).

|===

[#_pipeline_step_inbound_native_collection_type_options]
=== Pipeline Step Inbound Native Collection Type Options
The following options are available on the `nativeCollectionType` pipeline element:

.Step Inbound Native Collection Type Location
[source,json]
----
{
    "steps": [
        {
            "inbound": {
                "nativeCollectionType":{
                    "..."
                }
            }
        }
    ]
}
----
.Step Inbound Native Collection Type Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/inbound/nativeCollectionType/name`
| Yes
| None
| This is the name of the xref:dictionary-metamodel.adoc[dictionary] type to be used as the inbound native collection type.

| `steps/step/inbound/nativeCollectionType/package`
| No
| Yes
| This is the package for the xref:dictionary-metamodel.adoc[dictionary] to look up the inbound native collection type.
If not specified, it will default to the base package.
|===

[#_pipeline_step_inbound_record_type_options]
=== Pipeline Step Inbound Record Type Options
The following options are available on the `recordType` pipeline element:

.Step Inbound Record Type Location
[source,json]
----
{
    "steps": [
        {
            "inbound": {
                "recordType":{
                    "..."
                }
            }
        }
    ]
}
----
.Step Inbound Record Type Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/inbound/recordType/name`
| Yes
| None
| This is the name of the `record` to be used as the inbound data type.

| `steps/step/inbound/recordType/package`
| No
| Yes
| This is the package in which the `record` to be used as the inbound data type resides.
If not specified, it will default to the base package.
|===


[#_pipeline_step_outbound_options]
=== Pipeline Step Outbound Options
The following options are available on the `outbound` pipeline element:

.Step Outbound Location
[source,json]
----
{
    "steps": [
        {
            "outbound": {
                "..."
            }
        }
    ]
}
----
.Step Outbound Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/outbound/type`
| Yes
| No
| Allows specification of how the step returns results. There are currently three options:

* xref:messaging-details.adoc[`messaging`] - a message will be emitted from the step. This message can contain content,
a pointer to content, or just a signal.
* `native` - results will be directly returned from the step to be handled by the caller of the step. As such, messaging
inbound cannot be combined with a native outbound since no caller will exist.
* `void` - no outbound result is specified. All other options should be removed if using void or, preferably, the entire 
outbound block should be eliminated from your step.

| `steps/step/outbound/channelName`
| Situational
| None
| Required if using xref:messaging-details.adoc[messaging] as the `outbound` type, `channelName` specifies the messaging
channel to which output should be sent.

| `steps/step/outbound/nativeCollectionType` (xref:#_pipeline_step_outbound_native_collection_type_options[details])
| No
| Yes
| If using native as the `outbound` type, `nativeCollectionType` allows the implementation of the collection object being 
returned from the step to be customized to any valid xref:type-metamodel.adoc[Type Manager] type. If
not specified, it will default to dataset (which in turn is defaulted to `org.apache.spark.sql.Dataset` for a 
Spark-based implementation).

This has been changed to be defined to a valid dictionary type.

| `steps/step/outbound/recordType` (xref:_pipeline_step_outbound_record_type_options[details])
| No
| Yes
| Allows the type of an individual record being returned from a step to be defined to any valid
xref:type-metamodel.adoc[Type Manager] type. If not specified, it will default to row (which in turn
is defaulted to `org.apache.spark.sql.Row` for a Spark-based implementation).

This has been changed to be defined to a valid `record` type.
|===

[#_pipeline_step_outbound_native_collection_type_options]
=== Pipeline Step Outbound Native Collection Type Options
The following options are available on the `nativeCollectionType` pipeline element:

.Step Outbound Native Collection Type Location
[source,json]
----
{
    "steps": [
        {
            "outbound": {
                "nativeCollectionType":{
                    "..."
                }
            }
        }
    ]
}
----
.Step Outbound Native Collection Type Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/outnbound/nativeCollectionType/name`
| Yes
| None
| This is the name of the xref:dictionary-metamodel.adoc[dictionary] type to be used as the outbound native collection
type.

| `steps/step/outbound/nativeCollectionType/package`
| No
| Yes
| This is the package for the xref:dictionary-metamodel.adoc[dictionary] to look up the outbound native collection type.
If not specified, it will default to the base package.
|===

[#_pipeline_step_outbound_record_type_options]
=== Pipeline Step Outbound Record Type Options
The following options are available on the `recordType` pipeline element:

.Step Outbound Record Type Location
[source,json]
----
{
    "steps": [
        {
            "outbound": {
                "recordType":{
                    "..."
                }
            }
        }
    ]
}
----
.Step Outbound Record Type Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/outbound/recordType/name`
| Yes
| None
| This is the name of the `record` to be used as the outbound data type.

| `steps/step/outbound/recordType/package`
| No
| Yes
| This is the package in which the `record` to be used as the outbound data type resides.
If not specified, it will default to the base package.
|===

[#_pipeline_step_persist_options]
=== Pipeline Step Persist Options
The following options are available on the `persist` pipeline element:

.Step Persist Location
[source,json]
----
{
    "steps": [
        {
            "persist": {
                "..."
            }
        }
    ]
}
----
.Step Persist Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/persist/type`
| Yes
| None
| Allows the specification of the storage system that you want to use to persist data in your step. There are currently
five options:

* `delta-lake` - leverage https://delta.io/[Delta Lake,role=external,window=_blank] to save data; this is the preferred, 
general purpose data store that is well suited for intermediate storage that will consumed by subsequent steps within 
Spark implementations.
* `hive` - leverage https://hive.apache.org/[Apache Hive,role=external,window=_blank] to save data; this is often a 
good choice when you want to expose data for remote consumption.
* `rdbms` - leverage https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html[Rdbms,role=external,window=_blank] to save data.
* `elasticsearch` - leverage https://www.elastic.co/[Elasticsearch,role=external,window=_blank] to save data.
* `neo4j` - leverage https://neo4j.com/[Neo4j,role=external,window=_blank] to save data.

| `steps/step/persist/mode`
| No
| Yes
| Allows the specification of how you want to persist data in your step. There are currently four options:

* `append`
* `error`
* `ignore`
* `overwrite`

If not specified, it will default to `append`. Please see
https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes[documentation on Spark save
modes] for details on the options.

| `steps/step/persist/collectionType` (xref:_pipeline_step_persist_collection_type_options[details])
| No
| Yes
| Allows the implementation of the collection object being persisted from the step to be customized to
any valid xref:type-metamodel.adoc[Type Manager] type. If not specified, it will default to dataset
(which in turn is defaulted to `org.apache.spark.sql.Dataset` for a Spark-based implementation).

| `steps/step/persist/recordType` (xref:_pipeline_step_persist_record_type_options[details])
| No
| Yes
| Allows the type of an individual record that will be persisted from a step to be defined to any valid
xref:type-metamodel.adoc[Type Manager] type. If not specified, it will default to row (which in turn
is defaulted to `org.apache.spark.sql.Row` for a Spark-based implementation).

|===


[#_pipeline_step_persist_collection_type_options]
=== Pipeline Step Persist Collection Type Options
The following options are available on the `collectionType` pipeline element:

.Step Persist Collection Type Location
[source,json]
----
{
    "steps": [
        {
            "persist": {
                "collectionType":{
                    "..."
                }
            }
        }
    ]
}
----
.Step Persist Collection Type Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/persist/collectionType/name`
| Yes
| No
| This is the name of the xref:dictionary-metamodel.adoc[dictionary] type to be used as the persist collection type.

| `steps/step/persist/collectionType/package`
| No
| Yes
| This is the package for the xref:dictionary-metamodel.adoc[dictionary] to look up the persist collection type. If not
specified, it will default to the base package.
|===

[#_pipeline_step_persist_record_type_options]
=== Pipeline Step Persist Record Type Options
The following options are available on the `recordType` pipeline element:

.Step Persist Record Type Location
[source,json]
----
{
    "steps": [
        {
            "persist": {
                "recordType":{
                    "..."
                }
            }
        }
    ]
}
----
.Step Persist Record Type Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/persist/recordType/name`
| Yes
| None
| This is the name of the `record` to be used as the persist data type.

| `steps/step/persist/recordType/package`
| No
| Yes
| This is the name of the `record` to be used as the persist data type.

|===

[#_pipeline_step_provenance_options]
=== Pipeline Step Provenance Options
The following options are available on the `provenance` pipeline element:

.Step Provenance Location
[source,json]
----
{
     "steps": [
        {
            "provenance": {
                "..."
            }
        }
    ]
}
----
.Step Provenance Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/provenance/enabled`
| Yes
| None
| Setting `enabled` to false will disable provenance creation.

| `steps/step/provenance/resource`
| No
| None
| The name for the resource being modified in the step.

| `steps/step/provenance/subject`
| No
| None
| The name of the subject modifying the resource in the step.

| `steps/step/provenance/action`
| No
| None
| The name of the action being taken on the resource in the step.

|===

[#_pipeline_step_alerting_options]
=== Pipeline Step Alerting Options
The following options are available on the xref:alerting-details.adoc[`alerting`] pipeline element:

.Step Alerting Location
[source,json]
----
{
    "steps": [
        {
            "alerting": {
                "..."
            }
        }
    ]
}
----
.Step Alerting Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/alerting/enabled`
| Yes
| None
| Setting `enabled` to false will disable alerting.

|===

[#_pipeline_step_post_actions_options]
=== Pipeline Step Post Actions Options
[IMPORTANT]
The `postActions` pipeline step element is only applicable to a machine-learning training step!

The following options are available on the xref:post-actions.adoc[`postActions`] pipeline step element:

.Step Post Actions Location
[source,json]
----
{
    "steps": [
        {
            "postActions": [
                {
                    "..."
                }
            ]
        }
    ]
}
----
.Step Post Actions Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/postActions/name`
| Yes
| None
| Specifies the name of the xref:post-actions.adoc[post-training action]. It should represent the functional purpose of
your post action and use UpperCamelCase/PascalCase notation (e.g., `ConvertModel`).

| `steps/step/postActions/type`
| Yes
| None
| Specifies the type of the xref:post-actions.adoc[post-training action]. The following types are currently supported:

* `model-conversion` - to convert the trained model to another model format.
* `freeform` - to implement a custom post-training process.

| `steps/step/postActions/modelTarget`
| Situational
| None
| Required when post action type is `model-conversion`. Specifies the format to convert the trained model to. The
following model targets are currently supported:

* `onnx` - to convert a model to ONNX format. Please see https://github.com/onnx/onnxmltools[ONNX
documentation,role=external,window=_blank] for more information.
* `custom` - to implement a custom model conversion.

| `steps/step/postActions/modelSource`
| Situational
| None
| Required when post action type is `model-conversion`. Specifies the format of the trained model that will be converted.

For `onnx` model conversion, the following model sources are currently supported:

* `sklearn`
* `keras`

|===

[#_pipeline_step_configuration_options]
=== Pipeline Step Configuration Options
The following options are available on the `configuration` pipeline element:

.Step Configuration Location
[source,json]
----
{
    "steps": [
        {
            "configuration": [
                {
                    "..."
                }
            ]
        }
    ]
}
----
.Step Configuration Metamodel Options
[cols="2a,1a,1a,4a"]
|===
| Element Name | Required? | Default | Use

| `steps/step/configuration/key`
| No
| None
| The name of the configuration key-value pair by which the value can be retrieved.

| `steps/step/configuration/value`
| No
| None
| The configuration value.

|===