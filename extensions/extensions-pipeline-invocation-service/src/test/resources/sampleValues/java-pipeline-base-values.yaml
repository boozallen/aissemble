metadata:
    name: java-pipeline
    namespace: default
spec:
  type: Java
  mode: cluster
  image: "boozallen/test-project-spark-worker-docker:latest"
  imagePullPolicy: IfNotPresent
  mainClass: validation.test.project.pipeline.JavaPipelineDriver
  mainApplicationFile: "local:///opt/spark/jobs/pipelines/java-pipeline.jar"
  deps:
    packages:
      - mysql:mysql-connector-java:8.0.30
      - org.apache.hadoop:hadoop-aws:3.3.4
      - com.amazonaws:aws-java-sdk-bundle:1.12.262
      - io.delta:delta-core_2.12:2.4.0
      - io.delta:delta-storage:2.4.0
    excludePackages: []
  hadoopConf:
    fs.s3a.fast.upload: "true"
    fs.s3a.path.style: "true"
  dynamicAllocation:
    enabled: true
    initialExecutors: 0
    minExecutors: 0
    maxExecutors: 4
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    serviceAccount: spark
    env:
      - name: KRAUSENING_BASE
        value: /opt/spark/krausening/base
      - name: AWS_ACCESS_KEY_ID
        value: "123"
      - name: AWS_SECRET_ACCESS_KEY
        value: "456"
      - name: STORAGE_ENDPOINT
        value: "http://s3-local:4566"
    javaOptions: "-DKRAUSENING_BASE=/opt/spark/krausening/base"
  executor:
    cores: 1
    memory: "512m"
    env:
      - name: KRAUSENING_BASE
        value: /opt/spark/krausening/base
      - name: AWS_ACCESS_KEY_ID
        value: "123"
      - name: AWS_SECRET_ACCESS_KEY
        value: "456"
      - name: STORAGE_ENDPOINT
        value: "http://s3-local:4566"
    javaOptions: "-DKRAUSENING_BASE=/opt/spark/krausening/base"
